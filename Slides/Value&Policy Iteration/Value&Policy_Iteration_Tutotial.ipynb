{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The meaning of the equation\n",
    "\n",
    "$ p_{i,j}^{\\pi} = \\sum_a \\Pr[A_t=a \\mid S_t=i, \\pi] ; \\Pr[S_{t+1}=j \\mid S_t=i, A_t=a] $\n",
    "This means:\n",
    "> The probability of going from state **i** to state **j** under policy **π** equals\n",
    "> the sum over all possible actions **a** of:\n",
    ">\n",
    "> * The probability that the **policy** chooses action **a** in state **i**\n",
    "> * times the probability that the **environment** moves to **j** if you took action **a** from **i**\n",
    "\n",
    "So:\n",
    "* **Policy choice:** `π[a|i]` = how likely the agent chooses action `a` in state `i`\n",
    "* **Environment behavior:** `P[a][i][j]` = how likely the environment moves to `j` after doing action `a` in `i`\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2. In matrix form\n",
    "\n",
    "We can combine all ( $ p_{i,j}^{\\pi} $ ) into a big **matrix $( P^\\pi )$** of size `[n_states × n_states]`.\n",
    "Each entry represents the **probability of going from i → j** under that policy.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4. Interpretation\n",
    "\n",
    "* Each row of `P_pi` now represents how the environment behaves **after combining** your policy with the stochastic transitions.\n",
    "* This lets you treat the entire process as a **Markov Reward Process (MRP)**:\n",
    "  $\n",
    "  V^\\pi = R^\\pi + \\gamma P^\\pi V^\\pi\n",
    "  $\n",
    "  which you can then solve or evaluate.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5. Summary\n",
    "\n",
    "| Symbol       | Meaning                                                | Python representation                           |            |\n",
    "| :----------- | :----------------------------------------------------- | :---------------------------------------------- | ---------- |\n",
    "| (π[a         | i])                                                    | Probability of choosing action `a` in state `i` | `pi[s, a]` |\n",
    "| (P[a][i][j]) | Probability of going to `j` from `i` taking action `a` | `P[a, s, j]`                                    |            |\n",
    "| (P^{π}[i,j]) | Overall probability of `i→j` under policy `π`          | `P_pi[i, j] = Σ_a π[s,a] * P[a,s,j]`            |            |\n",
    "\n",
    "---"
   ],
   "id": "408999bdd80369a0"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-09T03:08:33.024136Z",
     "start_time": "2025-10-09T03:08:32.481891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Suppose:\n",
    "nS, nA = 3, 2  # 3 states, 2 actions\n",
    "\n",
    "# Environment transition probabilities: P[a][s][s']\n",
    "P = np.array([\n",
    "    [[0.8, 0.2, 0.0],   # action 0 transitions\n",
    "     [0.1, 0.6, 0.3],\n",
    "     [0.0, 0.3, 0.7]],\n",
    "\n",
    "    [[0.9, 0.1, 0.0],   # action 1 transitions\n",
    "     [0.2, 0.7, 0.1],\n",
    "     [0.5, 0.2, 0.3]]\n",
    "])\n",
    "\n",
    "# Policy π[s,a] — probability of choosing each action in each state\n",
    "pi = np.array([\n",
    "    [0.7, 0.3],  # in state 0, 70% choose a0, 30% choose a1\n",
    "    [0.5, 0.5],\n",
    "    [0.1, 0.9]\n",
    "])\n",
    "\n",
    "# Compute P^π = sum_a π[s,a] * P[a][s,:]\n",
    "P_pi = np.einsum('sa,asj->sj', pi, P)  # vectorized expectation over actions\n",
    "\n",
    "print(\"P^π =\\n\", P_pi)\n",
    "print(\"Each row sums to:\", P_pi.sum(axis=1))"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P^π =\n",
      " [[0.83 0.17 0.  ]\n",
      " [0.15 0.65 0.2 ]\n",
      " [0.45 0.21 0.34]]\n",
      "Each row sums to: [1. 1. 1.]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Value Iteration\n",
    "\n",
    "1. Set $v_0 = [0, \\ldots, 0]$\n",
    "2. For $i = 0, 1, 2, \\ldots$\n",
    "\n",
    "   - For all states $s$, update\n",
    "     $$\n",
    "     v_{i+1}(s) = \\max_a \\Big( \\mathcal{R}_s^a + \\gamma \\sum_{s'} P(s'|s,a) \\, v_i(s') \\Big)\n",
    "     $$\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "- $v_i(s)$ → best total reward achievable within *i* steps.\n",
    "- $\\mathcal{R}_s^a$ → immediate reward for taking action *a* in state *s*.\n",
    "- $P(s'|s,a)$ → probability of reaching *s′* given *(s,a)*.\n",
    "- $\\gamma$ → discount factor (how much we value future rewards).\n",
    "\n",
    "---\n"
   ],
   "id": "afad2e53c74983b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "V = np.zeros(n_states)\n",
    "for i in range(max_iters):\n",
    "    V_new = np.zeros_like(V)\n",
    "    for s in range(n_states):\n",
    "        V_new[s] = max(R[s,a] + gamma * sum(P[a][s][s2] * V[s2] for s2 in range(n_states))\n",
    "                       for a in range(n_actions))\n",
    "    if np.max(np.abs(V_new - V)) < tol:\n",
    "        break\n",
    "    V = V_new"
   ],
   "id": "9434aa96a3b32482"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
